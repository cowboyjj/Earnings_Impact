\documentclass[%
 aip,
 jmp,%
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{amsmath}
%\usepackage{multicol}

%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\usepackage[backend=biber, sorting=none]{biblatex}
%\usepackage[backend=biber, sorting=ynt]{biblatex}

\addbibresource{references.bib} %Imports bibliography file


\begin{document}


\title{Capturing dynamics of post-earnings-announcement drift using genetic algorithm-optimized supervised learnings}% Force line breaks with \\
\thanks

\author{Zhengxin Joseph Ye}
\affiliation{Department of Computing, Imperial College}%Lines break automatically or can be forced with \\
\author{Thomas Heinis}
\affiliation{Department of Computing, Imperial College}
\author{Pantelis J. Beaghton}
\affiliation{Department of Computing, Imperial College}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

% ==== useful materials ====
%https://www.verbosus.com/bibtex-style-examples.html
%https://www.tablesgenerator.com/
%https://stackoverflow.com/questions/1673942/latex-table-positioning
% ==== useful materials ====

\begin{abstract}
Post-Earnings-Announcement Drift (PEAD) is a stock market phenomenon when a stock's cumulative abnormal returns have a tendency to drift in the direction of an earnings surprise for weeks following an earnings announcement. Although it is one of the most studied stock market anomalies and its existence is well understood, the current literature is limited in explaining this phenomenon by a small number of factors using simpler regression methods and hasn't been able to accurately perform prediction on it using known factors. In this paper we aim to use supervised learning models to capture the dynamics of stock' quarterly PEAD using a wider range of both fundamental and technical factors. We test a deep neural network (Multiplayer Perceptron), an extreme gradient boosting model (XGBoost) as well as support vector machines (SVM) with different kernels on quarterly earning announcement data from 1106 companies from the Russell 1000 index between 1997 and 2018. Our experiments show that XGBoost performs better in PEAD predictions of out-of-sample test stocks than MLP and SVM in a range of experiments. More significantly, since we put the focus of our experiments and analysis at the portfolio level, for the first time in the literature we've produced post-earnings stock return predictions that are strongly correlated with return of portfolios consisting of out-of-sample test stocks. Our methods and results consistently show that we can use out-of-sample stocks to form portfolios with the highest positive-returning portfolios to long and lowest negative-returning portfolios to short in the process of constructing market neutral strategies.

\end{abstract}

%\pacs{Valid PACS appear here}% PACS, the Physics and Astronomy
                             % Classification Scheme.
%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle


%\tableofcontents

%\begin{quotation}
%The ``lead paragraph'' is encapsulated with the \LaTeX\ 
%\verb+quotation+ environment and is formatted as a single paragraph before the first section heading. 
%(The \verb+quotation+ environment reverts to its usual meaning after the first sectioning command.) 
%Note that numbered references are allowed in the lead paragraph.
%The lead paragraph will only be found in an article being prepared for the journal \textit{Chaos}.
%\end{quotation}
%

\section{Introduction}

The stock market is characterized by nonlinearities, discontinuities, and multi-polynomial components because it continuously interacts with many factors such as individual company's news, political events, macro economic conditions, and general supply and demand, etc \cite{Gocken2016}. The non-stationary nature of the stock market is supported by a widely believed but still hotly contested economic theory Efficient Market Hypothesis which states that asset prices fully reflect all available information and the market only moves by reacting to new information. Such a theory implies that the stock market behaves like a martingale and knowledge of all past prices is not informative regarding the expectation of future prices.

Ball and Brown \cite{Ball1968} were the first to note that after earnings are announced, estimated cumulative abnormal returns continue to drift up for firms that are perceived to have reported good financial results for the preceding quarter and drift down for firms whose results have turned out worse than the market had expected. The discovery of Post Earnings Announcement Drift, which is a violation of semi-strong Efficient Market Hypothesis, seems to suggest that while stock markets are generally efficient, there may be information leakages around the announcement dates, coupled with post-earnings drift, resulting in price movement anomalies. It also seems to suggest that past stock price information or other past economic or financial information can potentially be used to predict price movement following an earnings announcement. 

We have noticed that a lot of researches on PEAD came out in the late 1980s and 1990s. Fama and French \cite{Fama1993} shows that average stock returns co-vary with three factors, namely, the market risk factor, the book-to-market factor, and the size factor. Bhushan suggests that the existence of sophisticated and unsophisticated investors, transaction costs and economies of scale in managing money can explain the market's delayed response to earnings \cite{Bhushan1994}. Nearly all previous research has pooled companies with negative and positive earnings surprises when measuring the effect of earnings surprises on abnormal returns and regress the absolute value of earnings surprise as well as other factors against the absolute value of abnormal return \cite{Qiu2014}. However, we believe that stock markets don't react symmetrically to negative and positive earnings surprises and there are a lot more factors in play that drive the near term risk adjusted returns of a stock following an earnings release. 

By using machine learning models we have manged to leap straight to the more important goal of predicting the direction of PEAD. In this process we've overcome a number of constraints commonly seen in previous researches: we are including a much wider range of factors including both fundamental and technical/momentum factors; we achieve a higher level of generality without having to pre-group companies by the value of their earnings surprises or other attributes prior to the analysis or prediction [*]. Additionally we've chosen 1106 stocks that are or once existed as components of the Russell 1000 index (which tracks approximately the 1000 largest public companies in the US) during the chosen test time period between 1997 and 2018. Our selection includes companies that either went bankrupt or dropped out of Russell 1000, significantly reducing survivorship bias in our training data. This test population is larger than a lot of previous studies of similar nature. For example Beyaz and co only chose 140 stocks from S\&P500 when they attempted to forecast stock prices both six months and a year out based on fundamental analysis and technical analysis \cite{Beyaz2018} and Bradbury used a sample of only 172 firms to research the relationships among voluntary semi-annual earnings disclosures, earnings volatility, unexpected earnings, and firm size \cite{Bradbury1992}.

Recognising the highly nonlinear nature of stock price movements, we've chosen a variety of supervised learning models in search of one which can best work through the high noises embedded in the price data. We've experimented with a deep neural network, a varieties of support vector machines with different kernels, and an Extreme Gradient Boosting (XGB) model. In our experiments with all these models, we divide the training data into in-sample and out-of-sample periods of varying lengths and use the in-sample data set to tune a model's hyperparameters. Our early experiments show that a traditional grid search way of finding optimal parameter set is inexhaustive and can be very slow. Instead we've chosen to use the highly adaptable Genetic Algorithm to tune our models [*]. Since the search range and granularity of each model's tunable hyperparameters examined by the Genetic Algorithm are often unknown beforehand and they directly determine the complexity of the resulting model, they must be chosen sensibly. Searching with a limited sets of parameter will result in a nonoptimal model that is not able to fit the essential structure of the training data set. To avoid that potential problem we have chosen a broad range and small step for each of the hyperparameters. Lastly we employ a 5-fold cross validation (CV) within each Genetic Algorithm iteration for estimating the optimal combination of each model's hyperparameters.  
 
\section{Related Work}
Since the discovery of Post Earnings Announcement Drift as a stock market anomaly by Ball and Brown \cite{Ball1968} who documented the return predictability for up to two months after the annual earnings announcements, an extensive research has been carried out in the literature though with varying results. For example Foster, Olsen and Shevlin \cite{Foster1984} found systematic post-announcement drifts in security returns are only found for a subset of earnings expectations models when testing drifts in the [+1, +60] trading day period. In recent years the literature has become less limited to the specific study of PEAD and instead put more focus on the direct predictions of stock price movement using stocks' fundamental and/or technical information, again with varying rates of success. Malkiel studied the impact of price/earnings (P/E) ratios and dividend yields on stock prices using the Campbell-Shiller model. He conceded his work demonstrated that exploitable arbitrage didn't exist for investors to earn excess risk-adjusted returns and he could not find a market timing strategy capable of producing returns exceeding buying and hold a broad market index \cite{Malkiel2004}. Olson and Mossman on the other hand not only showed that artificial neural network outperforms traditional regression based methods when forecasting 12-month returns by examining 61 financial ratios for 2352 Canadian stocks but more importantly shows that by using fundamental metrics sourced from earning reports they were able to achieve excessive risk-adjusted returns \cite{Olson2003}.

Other authors went beyond metrics from earnings reports and attempted stock forecast using both fundamental and technical analysis. Sheta, et al. explored the use of ANN, SVM and Multiple Linear Regression for prediction of S\&P500 market index. They selected 27 technical indicators as well as macro economic indicators and reported that SVM contributed to better predictions than the other models tested \cite{Sheta2015}. Hafezi et all considered both fundamental and technical analyses in a novel model called Bat-neural Network Multi-agent System when forecasting stock returns. The resulted MAPE statistic showed that the new model performed better than typical Neural Network coupled with Genetic Algorithm \cite{Hafezi2015}. 

When it comes to selecting machine learning models for event driven stock price forecast the literature has looked a lot at Support Vector Machines. Zhang constructed a novel ensemble method integrated with AdaBoost algorithm, probabilistic Support Vector Machine and Genetic Algorithm and verified its performance over 20 shares from the SZSE and 16 stocks from NASDAQ. He showed the new ensemble method achieved preferable profit in simulation of stock investment \cite{Zhang2016}. Madge used daily closing price for 34 technology stocks on a SVM model with radial kernel to calculate price volatility and momentum for individual stocks and for the overall sector. The model attempts to predict whether a stock price sometime in the future will be higher or lower than it is on a given day. They found little predictive ability in the short-run but definite predictive ability in the long-run \cite{Madge2015}. We have not found any creditable research on stock forecast using XGBoost and we are contributing to the literature for that.

\textbf{\textit{Joseph's note to tutors (to be removed):
I have conducted literature review on other individual technical topics such as Genetic Algorithm and Cross Validation as well as stock price forecasting under other settings and scenarios. I will be able to extend the literature review section into these areas in the real paper if necessary.}}

\section{Model Features Generation}
We have chosen in total 1106 Russell1000 companies for analysis. The chosen time frame is between the fourth financial quarter of 1997 (1997 Q1) and the fourth financial quarter of 2018 (2018 Q4). While the output of the supervised models is risk-adjusted near term stock movements (\%change) of individual companies following each quarter's earnings announcement, the input to our models consists of the following sets of unadjusted data which we've sourced from Bloomberg:
\begin{itemize}
\item Financial statements data
\item Earnings Surprise data
\item Price movements data
\item Momentum indicator data
\item Short interest data
\end{itemize}

In total we've sourced 97901 quarterly financial statements from our chosen companies over the test time frame. The final population of valid data points used for training and testing whose input features include both financial statement metrics and other economic metrics stands in the region of 40,000 to 50,000, depending on the test cases. There are a number of reasons for the reduced population: (a) there are no Earnings data, Short interest data or other input feature data on Bloomberg for a good number of historical financial quarters within the test time frame; (b) we've discarded certain companies in certain historical quarters when the earnings reports suffered badly from missing data; (c) We've been very careful with whether an earnings report was released before market opened, after market closed or during trading hours as such a difference is significant and we've needed to alter the forecast starting point accordingly. Bloomberg is missing such information for some financial quarters in earlier years and we've discarded those quarters.

\subsection{\label{sec:level2}Financial Statements data}
Table 1 shows 24 metrics from earnings reports have been chosen to create training data.

\begin{table}[h]
\begin{tabular}{|p{4cm}|p{4cm}|}
\hline
Cash   & Operating Margin                           \\ \hline
Cash from Operating Activities                   & Price to Book Ratios                            \\ \hline
Cost of Revenue                & Price to Cashflow Ratios                  \\ \hline
Current Ratio                     & Price to Sales Ratios                   \\ \hline
Dividend Payout Ratio & Quick Ratio              \\ \hline
Dividend Yield               & Return On Assets                      \\ \hline
Free Cash Flow               & Return On Common Equity                       \\ \hline
Gross Profit                 & Revenue                  \\ \hline
Income from Continued Operations                  & Short Term Debt                           \\ \hline
Inventory Turnover                        & Total Asset                   \\ \hline
Net Debt to EBIT    & Total Asset \\ \hline
Net Income                & Total Debt to Total Assets                        \\ \hline
Operating Expenses                & Total Debt to Total Equity                        \\ \hline
Operating Income                & Total Inventory                        \\ \hline
                & Total Liabilities                         \\ \hline
\end{tabular}
\caption{\label{tab:table-name}Earnings report metrics chosen as input features}
\end{table}

Based on the reported value of these metrics we've engineered new features as quarterly change and yearly change of each of all the 29 report metrics. 

\subsection{\label{sec:level2}Earnings Surprise data}

Earnings Surprise represents how much a company's actual reported Earnings Per Share (EPS) is more (or less) than the average of a selected group of stock analysts' estimates on that quarter's EPS. We are not calculating Earnings Surprise as a \%change between the reported EPS and market estimated EPS because (a) \%change is too volatile as a very small change when the actual EPS levels is close to zero will lead to a misleading large \%change, and (b) we would like to avoid the change of signs problem when EPS turns from negative to positive or vice versa.

We've subsequently engineered the following three features related to Earnings Surprise:

\begin{itemize}
\item Current quarter's Earnings Surprise (reported EPS minus market estimated EPS); 
\item Difference between current quarter's Earnings Surprise and that of the previous quarter;
\item Difference between current quarter's Earnings Surprise and the average Earnings surprise of the preceding three quarters;
\end{itemize}

\subsection{\label{sec:level2}Momentum Indicators}
We've chosen the following technical/momentum indicator values calculated on the same day an individual company's quarterly earnings data was released:
\begin{itemize}
\item 9-day Relative Strength Index (RSI)
\item 30-day Relative Strength Index
\item 5-day Moving Average / 50-day Moving Average
\item 5-day Moving Average /200-day Moving Average
\item 50-day Moving Average / 200-day Moving Average
\end{itemize}
We believe all these indicators should in a way measure how a stock's recent short term movements compare to its historical movements further back in time. The inclusion of momentum indicators is to allow the prediction process of future stock movements to take into account a stock's recent movement trend as information leakage does happen prior to financial reportings.

\subsection{\label{sec:level2}Short Interest data}
Short interest ratio is released for most companies twice a month and is calculated by dividing the number of shares short in a stock by the stock's average daily trading volume. The short interest ratio is a good gauge on how heavily shorted a stock may be versus its trading volume. The most recent short interest ratio for each company prior to its earnings release is sourced as an input feature to the model for that company.

\section{Data Pre-processing}

With totally 1106 companies involved over a 21 years, there is a lot of data representing input features for each company at each quarter. In order for them to be understood by the models we put them into a matrix-like data structure $A \in M_{m \times n}( \mathbb{R} )$ where each of the $m$ rows represents an $n$ dimensional training data point, indexed by the pairing of a company name and a historical quarter , and each column holds data of the same feature from all the data points.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{"Data Pre-processing".png}
  \caption{Steps of Data Pre-processing}
  \label{fig:preprocessing}
\end{figure}

Before we put the data of all the companies and of all the quarters into a matrix, we pre-process each company's data to deal with outliers and to standardize data of every company. Firstly, we employ Winsorization \cite{Bin1998} to reduce the number of outliers present in the input features. This is carried out on the feature data of each individual company. Secondly, we standardize a selective group of features of each company. Every company's standardized features will then be stacked back into a full training data set. The rationale of standardizing the features at the company level can be explained in the following example. Certain blue chip large cap companies, or certain companies from non-cyclical sectors may have little variation in some of their financial metrics from quarter to quarter whereas a lot of mid cap growth companies may see more volatile movements in the value of the same financial metrics. Both cases are considered norm for respective companies but when we put data of these companies together and put them under the same feature column, we can make them more comparable from the models' perspective once different company's data have been standardized. The pre-processing process is illustrated in Figure \ref{fig:preprocessing}.

\section{Models and Methods}
In order to forecast post earnings price drift we've chosen to experiment with a deep neural network, an Extreme Gradient Boosting mdoel, and support vector machines with different kernels. 

\subsection{\label{sec:level2}Deep Neural Network}

Deep learning has been a cornerstone in machine learning for the last few decades. Deep neutral networks have been used to achieve a lot of state-of-the-art results and are playing a big part either on their owns in many fields such as image process or as part of other machine learning disciplines such as natural language process and reinforcement learning. In our experiments deep neural network whose final structure will be decided through the Genetic Algorithm optimization process, will serve as a benchmark model for SVM and XGBoost. Training neural network is a convex optimization problem with the loss function defined as:

\begin{equation}
L(\omega) \triangleq \sum_{i=1}^M L_{i}(\omega)
\end{equation}

 Where $L_{i}(\omega)$ is a loss function for data point $i \in \left\{1,2,...,M \right\}$ and $\omega$ are the model weights being optimized. A neural network is a nonlinear system due to the presence of an activation function on each neuron (except for the output neuron as it can be linear). An activation function $\sigma(x)$ can take a lot of forms such as sigmoid, tanh, ReLU, etc and it makes the output of a neuron look like $f(x) = \sigma (\omega^{T} X + b)$ with $X$ being inputs to the neuron and $b$ being the bias. To minimize the loss function, Stochastic Gradient Descend (SGD) and its varients (stochastic, batch and mini-batch SGD) are used to optimize these weights during the training process \cite{Keskar2017}. This is done in an iterative fashion and by using learning rate $\alpha$ and the Jacobian matrix of derivatives of the loss function with respect to all the model weights $\triangledown L(\omega) = \left(\frac{\partial L}{\partial \omega_{1}},\frac{\partial L}{\partial \omega_{2}}, ...,\frac{\partial }{\partial \omega_{N}} \right)$:
 
 \begin{equation}
\omega_{i} \mbox{ := } \omega_{i}-\alpha\triangledown L(\omega)
\end{equation}
 
% This website offers useful information for the presentation of DNN's mathematical form: https://ml4a.github.io/ml4a/how_neural_networks_are_trained/

\subsection{\label{sec:level2}Support Vector Machine}

Support Vector Machine was first invented by Vladimir Vapnik and his colleagues in 1963 with its current standard form $\epsilon$-SVM proposed by Cortes and Vapnik in 1995 \cite{Vapnik1995}. Unlike regression based methods which aim at minimising the error function, SVM finds a hypothesis function $f\left(x\right)$ which represents a hyperplane in the input feature space whose prediction output $\widehat{y_{i}}$ deviates away from the actually observed value $y_{i}$ by at most $\epsilon$. Its linear form can be simply written as

\begin{equation}
f\left(x,\omega\right)=\sum x_{j} \omega_{j} + b
\end{equation}

Seeking a flat hyperplane means minimizing $\omega$ and we can achieve this by finding the minimal norm value of $\omega$ , $\frac{1}{2} \parallel\omega\parallel^{2}$, effectively formulating it as a convex optimizing problem. Also to ensure optimization convergence one can use slack variables $\xi_{i},  \xi_i^*$  \cite{Vapnik1995} to introduce soft margin to the loss function which turns out in this form:

\begin{equation}
\mbox{Minimise } \frac{1}{2} \parallel\omega\parallel^{2} + C\sum_i^\ell\left(\xi_{i} + \xi_i^*\right) 
\end{equation}

\begin{equation}
\mbox{Subject to } 
\begin{cases}
\omega_{i} x_{i} + b - y_{i} \leq \epsilon + \xi_i  \\  
y_{i} - \omega_{i} x_{i} - b \leq \epsilon + \xi_i^*  \\ 
\xi_{i},  \xi_i^* \geq 0
\end{cases}
\end{equation}

% Majority of SVM knowledge described here is sourced from https://alex.smola.org/papers/2003/SmoSch03b.pdf and https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html

Where $C$ adjusts the balance between the flatness of the hypothesis $f$ and how much deviation further from $\epsilon$ can be tolerated. We call this loss function the $primal$ loss function. We can make the computation simpler by turning the primal loss function to its Lagrange $dual$ formulation whose solution provides a lower bound to the solution of the primal problem and the dual form is expressed as:

\begin{equation}
\mbox{Maximise } 
\begin{cases}
-\frac{1}{2}\sum_{i, j=1}^\ell (\alpha_{i}-\alpha_i^*)(\alpha_{j}-\alpha_j^*)\langle x_{i}, x_{j} \rangle  \\  
-{\epsilon}  \sum_{i=1}^\ell y_{i} (\alpha_{i}+\alpha_i^*) + \sum_{i=1}^\ell y_{i} (\alpha_{i}-\alpha_i^*)  \\ 
\xi_{i},  \xi_i^* \geq 0
\end{cases}
\end{equation}

\begin{equation}
\mbox{Subject to } \sum_{i=1}^\ell (\alpha_{i}-\alpha_i^*) = 0 \mbox{ and }  \alpha_{i},\alpha_i^*\in[0,C]
\end{equation}

Here $\langle x_{i}, x_{j} \rangle$ is only for the linear form of SVM. This part of the loss function can be extended to using a kernel function $K(x_{i},x_{j})=\langle \psi(x_{i}), \psi(x_{j})   \rangle$ when a linear model can not adequately describe a regression problem where $\psi(x)$ is a transformation that maps x to a high-dimensional space. In our experiments we've tested the linear kernel as well as a sigmoid kernel, four polynomial kernels with degrees from 2 to 5 and a Radial Basis Function (RBF) kernel:

\begin{align*} 
\mbox{Signmoid: } & K(x_{i},x_{j}) = tanh(\alpha x_{i}^{T} x_{j} + c)  \\  
\mbox{Polynomial: } & K(x_{i},x_{j}) = {(x_{i}^{T} x_{j} + 1)}^{d}  \\  
\mbox{RBF: } & K(x_{i},x_{j}) = exp(-\frac{1}{2\sigma^{2}} {\parallel x_{i}-x_{j} \parallel}^2)
\end{align*}

% A recommended Latex math formula editor: https://hostmath.com/

% Using align* can align equations in the same block. The ampersand symbol & determines where the alignment happens
% https://www.overleaf.com/learn/latex/Aligning%20equations%20with%20amsmath

% To include texts as part of a math formula, simply use \mbox{}
% https://www.overleaf.com/learn/latex/Questions/Including_text_within_equations_in_LaTeX

% Here is a collection of SVM kernel formulae: https://data-flair.training/blogs/svm-kernel-functions/

\subsection{\label{sec:level2}Extreme Gradient Boosting} 

%https://arxiv.org/pdf/1603.02754.pdf
%https://xgboost.readthedocs.io/en/latest/tutorials/model.html
%https://medium.com/@vikeshsingh37/math-behind-gbm-and-xgboost-d00e8536b7de
%https://www.kdnuggets.com/2018/08/unveiling-mathematics-behind-xgboost.html

Extreme Gradient Boosting (XGBoost) is a scalable machine learning system for tree boosting invented by Tianqi Chen \cite{ChenTianqi2016} which has gained much prominence in recent years. It distinguishes itself from other existing tree boosting methods \cite{Tyree2011} \cite{Ye2009} by having cache-aware and sparsity-aware learnings. The former technology gives the system twice the speed against running a non-cache-aware but otherwise identical greedy tree splitting algorithm and the latter gives an amazing 50 times speed boosting against a naive implementation handling an Allstate-10k dataset \cite{ChenTianqi2016}. More importantly XGBoost has achieved algorithmic optimizations by introducing regularized learning objective within a tree structure which helps achieve smart tree splitting and branch pruning.

For a data set in matrix form $A \in M_{m \times n}( \mathbb{R} )$ with $m$ data points and $n$ features, a tree ensemble model uses $K$ base leaner functions to predict the output:

\begin{equation}
\widetilde{y_{i}} = \phi(x_{i})=\sum_{k=1}^K f_{k} (x_{i}), f_{k}\in \mathbb{F} 
\end{equation}

Where $\mathbb{F}$ is the space of regression trees. Each hypothesis $f_{k}$ corresponds to an independent tree structure $q$ with leaf scores $\omega$. XGBoost utilises regression trees each of which contains a score on each of its leaves. These scores help form the decision rules in the trees to classify each set of inputs into leaves and calculate the final predicted output by summing up the scores in the related leaves. Unlike other standard gradient boosting models such as AdaBoost and GBM which don't intrinsically perform regularization, XGBoost minimises a $regularized$ loss function in order to learn the set of functions:

\begin{equation}
L(\phi)=\sum_{i} \ell(\widehat{y_{i}},y_{i})+\sum_{k}\Omega(f_{k}) 
\label{func:xgboost_loss}
\end{equation}

Here $\ell$ is a differentiable convex loss function for the model output and the regularization term is defined as (though not limited to) $\Omega(f)=\gamma T + \frac{1}{2}\lambda {||\omega||}^{2}$  which reduces the chance of overfitting. As in a typical gradient tree boosting model a new base learner regression tree $f_{i}$ which most minimizes the loss function in equation \ref{func:xgboost_loss} is greedily and iteratively added to the final loss function. Let $\widehat{y}_{i,t}$ be the model output of the $i$-th instance at the $t$-th iteration the loss function can be re-written as

\begin{equation}
L_{t}(\phi)=\sum_{i,k} \ell(y_{i}, \widehat{y}_{i,t-1} + f_t(x_i))+\sum_{k}\Omega(f_{k,t}) 
\end{equation}

By taking the Taylor expansion on this loss function up to the second order and removing the constant terms as the result of the expansion the loss function can be simplied to:
\begin{equation}
L_{t}(\phi)=\sum_{j=1}^T [G_j \omega_j + \frac{1}{2}(H_j + \lambda) \omega_j^2] + \lambda T
\end{equation}

Where

\begin{align*}
& G_j = \sum_{i\in I_j} g_i \\  
& H_j = \sum_{i\in I_j} h_i \\  
& I_j=\left\{ i|q(x_i)=j \right\} \\  
& g_i =  \partial_{\widehat{y}_{i, t-1}} \ell(y_i, \widehat{y}_{i, t-1}) \\
& h_i =  \partial_{\widehat{y}_{i, t-1}}^2 \ell(y_i, \widehat{y}_{i, t-1})
\end{align*}

Here $T$ is the number of leaves in the tree. With $\omega_j$ being independent with respect to others, Tianqi \cite{ChenTianqi2016} has proven that the best $\omega_j$ for a given tree structure $q(x)$ should be

\begin{equation}
\omega_j^* = - \frac{G_j}{H_j + \lambda}
\end{equation}

which in turn makes the objective function come to its final form:
\begin{equation}
L_j^* =-\frac{1}{2}\sum_{j=1}^T \frac{G_j^2}{H_j + \lambda}+\gamma T
\end{equation}

Ideally the model would enumerate all possible tree structures with a quality score and pick the best one to be added iteratively. In reality this is intractable and optimization has to be done one tree level at a time. This is made available by the final form of the loss function as the model uses it as a scoring function to decide the optimal leaf splitting point. Assume that $I_L$ and $I_R$ are the instance sets of left and right nodes after the split. Letting $I = I_L  \cup  I_R$, the scoring function for leaf splitting is

\begin{equation}
L_{split} =\frac{1}{2} \left[ \frac{(\sum_{i\in I_{L}}g_i)^{2}}{\sum_{i\in I_{L}}h_i + \lambda}  + \frac{(\sum_{i\in I_{R}}g_i)^{2}}{\sum_{i\in I_{R}}h_i + \lambda} + \frac{(\sum_{i\in I}g_i)^{2}}{\sum_{i\in I}h_i + \lambda} \right] - \gamma
\end{equation}

These scores are then used by a method called the $exact$ $greedy$ $algorithm$ to enumerate all the possible splits for continuous features, allowing each level of a tree to be optimized and the overall loss function to be minimised in the process. When deployed on a distributed platform XGBoost employs approximate algorithms instead to alleviate the huge memory consumption demanded by the exact greedy algorithm although this is not needed in our experiments which run on a single machine.

\subsection{\label{sec:level2}Model Tuning}

The whole training data population is split into training set and test set. We have devised a number of test cases. In each case we forecast post earning cumulative abnormal returns on all the stocks that released data either in the same financial quarter or on the same date. Consequently the test set includes those stocks from the test quarter or test date and the training set includes all the data points prior to the test quarter or test date. The training set is used to tune the models with the help of Genetic Algorithm (GA) and cross validation (CV). Model tuning is one of the two most important steps (the other being data cleansing) in ensuring the model output can meaningfully capture the underlying dynamics of the dependent variable. In search for optimal hyperparameter sets we experimented a more straightforward approach of grid search but found it less effective in its performance and inexhuastive in the search results. Genetic Algorithm as an adaptable and easily extensible heuristic optimization method has been chosen to perform model tuning on all the selected models under experiment. Table II gives the list of hyperparameters of every model that we've put through GA for tuning:

\begin{table}[]
\begin{tabular}{|p{2.8cm}|p{2.8cm}|p{2.8cm}|}
\hline
\textbf{Deep Neural Network}       & \textbf{XGBoost}               & \textbf{Support Vector Machine}        \\ \hline
\textit{Number of epochs}          & \textit{Max depth}             & \textit{Kernel method}                 \\ \hline
\textit{Hidden layer neuron count} & \textit{Sub sample}            & \textit{Gamma}                         \\ \hline
\textit{Dropout rate}              & \textit{Column sample by tree} & \textit{C (model's penalty parameter)} \\ \hline
\textit{Regularization Lambda}     & \textit{Gamma}                 & \textit{Epsilon}                       \\ \hline
\textit{Learning rate}             & \textit{Learning Rate}         & \textit{}                              \\ \hline
\textit{Hidden layer count}        & \textit{Minimum child weight}  & \textit{}                              \\ \hline
\end{tabular}
\caption{\label{tab:table-name}Model hyperparameters optimized by GA + CV}
\end{table}



We would like to note that researchers in the literature typically focus on one or two kernel methods to go with the Support Vector Machine models. For instance Tay and Gao chosen Gaussian kernel with SVM to forecast financial time series [17] and Madge used Radial basis function (RBF) kernel in his attempt to forecast stock price movement [14]. Instead we've chosen 7 different kernels (including RBF, Sigmoid, Linear, and Polynomial of degrees 2 to 5) and use GA to optimize SVM's output accuracy out of all these kernels. This ensures we are not limited to a small number of common kernels like we've seen in the literature and instead we take full advantage of GA's optimization prowess to help us identify the best kernel and its accompanying model parameters for our SVM model. Similarly, when using multi-layer Neural Network researchers in the literature typically pre-fix the number of hidden layers or the number of neurons in each hidden layers for their models and only carry out model tuning on common hyperparameters such as dropout rate and learning rate. Again this practice can be subjected to sub-optimal model accuracy as the modeller has not included the model structure as part of the model optimization process and instead only focus on the hyper parameters of a predefined structure. Recognising the deficiency of this model calibration process we are including the number of hidden layers and the number of neurons in each hidden layer as our tuning targets effectively tuning both the Neural Network model structure as well as model hyperparameters simultaneously.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{"Genetic Algorithm + CV + Tuning".png}
  \caption{Hyperparameter Tuning using GA + CV}
  \label{fig:HyperTuning}
\end{figure}

%https://www.overleaf.com/learn/latex/Inserting_Images

When tuning a model each of the tuning targets is randomly initialised according to its own valid range of values. This initialization is repeated 40 times so that we have 40 sets of randomly initialised hyperparameters to start the GA process with. Each set is called a 'population' and each hyperparameter within a set is called a 'chromosome'. All of the 40 populations are considered to be part of the current 'generation'. The GA process carries out a 5-fold cross-validation on a model using each of the 40 populations and when finished, keep the 20 populations that have produced the smallest fitness values in the cross validation step. These 20 sets or 'populations' of hyperparameters are considered to have performed better in forecasting post-announcement drifts with the current model than the 20 discarded ones. These 20 better populations are then used to 'cross-breed' into 20 new populations and in this process 'mutation' is allowed to happen to the cross-bred populations, i.e. chromosomes in the 20 newly created populations are allowed to randomly change value following a predefined level of probability. At the end of this process we have produced a new and potentially better set of 40 populations of hyperparameters and we call them the new generation. The new generation are then fed through a second iteration of the GA process until eventually the minimum fitness value produced by the cross-validation step no longer changes its value within tolerance and at this point we've arrived at the optimal set of tuning targets which produces the smallest fitness value when being used in the current model.
Figure \ref{fig:HyperTuning}  shows how Genetic Algorithm and Cross Validation work together to produce the set of hyperparameters of each model which result in the highest prediction accuracy (smallest fitness value) on the validation set.


\section{Results}

Having prepared and engineered a wide range of feature data, we carefully tune a deep Neural Network, an Extreme Gradient Boosting model, as well as Support Vector Machine (SVM) models of different kernels. Our first experiment is to forecast 30 day post-earnings Cumulative Abnormal Return (CAR) as a measure of risk adjusted stock price return. The purpose of this experiment is to measure which model may perform better than other ones and we will then choose this same model for other experiments. An abnormal return is between the actual return of a security and its expected rate of return.

\begin{equation}
AR_{it} = r_{it} - E(r_{it})
\end{equation}

Where $AR_{it}$ is the one-day abnormal return for company $i$ on day $t$, $r_{it}$ is the actual one-day stock return and $E(r_{it})$ is the expected return of stock $i$. As explored by Kim \cite{Kim2003} there are a variety ways of evaluating the expected return including using quantitative models such as the one-factor CAPM model and the Fama French three-factor model \cite{Fama1993}. In our experiments we choose to use the S\&P500 index return to represent the broader market's return and use that to proxy a stock's expected return. Consequently our model output for stock $i$ is simply

\begin{equation}
AR_{i} =\sum_{t=1}^n (r_{it} - E(r_{it}))
\end{equation}

We have chosen to perform this test on all the stocks that filed for 2018 Q4 quarterly earnings with U.S. Securities and Exchange Commission (SEC). That means all the companies in the 83 quarters between and including 1997 Q1 and 2018 Q3, totalling 47367 data points, are used as the training data points, whereas the 924 data points from 2018 Q4 are used for out-of-sample testing. Although this is a regression problem because we forecast the CAR directly, we also examine the classification success rate by checking if both a predicted return and an actual return are of the same sign. As demonstrated by the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) error metrics in Table \ref{tab:predictionerror} and the classification success rate in table \ref{tab:classification}, XGBoost has significantly outperformed both the deep Neural Networks and SVM in forecasting 30 day Cumulative Abnormal Return immediately after each company released its financial statements. As a result XGBoost has been chosen to conduct a series of subsequent experiments.

\begin{table}[]
\begin{tabular}{p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\cline{2-3}
                                   & \textbf{MAE} & \textbf{RMSE} \\ \hline
\multicolumn{1}{|l|}{\textbf{XGB}} & 6.94\%       & 1.03\%        \\ \hline
\multicolumn{1}{|l|}{\textbf{MLP}} & 7.85\%       & 1.23\%        \\ \hline
\multicolumn{1}{|l|}{\textbf{SVM}} & 9.69\%       & 1.74\%        \\ \hline
\end{tabular}
\caption{\label{tab:predictionerror}Prediction error metrics by models}
\end{table}


\begin{table}[]
\begin{tabular}{l|c|}
\cline{2-2}
                                   & \textbf{Classification Success Rate} \\ \hline
\multicolumn{1}{|l|}{\textbf{XGB}} & 58\%                                 \\ \hline
\multicolumn{1}{|l|}{\textbf{MLP}} & 53\%                                 \\ \hline
\multicolumn{1}{|l|}{\textbf{SVM}} & 52\%                                 \\ \hline
\end{tabular}
\caption{\label{tab:classification}Average classification success rate by models}
\end{table}

As inspired by Chung's work \cite{Chung2011} who successfully evidenced that short term return predictability captures other factors, such as volatility, information asymmetry, investor sophistication, volume, size, and trading costs that affect arbitrage activities and the extent to which information is impounded in prices, we would like to explore the possibility of using post-earnings return predictability as a broader measure of market efficiency in the context of PEAD. Our machine learning based approach is in direct contrast to most earlier work in the literature as typified by \cite{Kim2003} which sought to pre-devise different portfolios by different characteristics of the factors under analysis and tried to analyse and determine the relationship between respective portfolios' return and the corresponding economic factors that segregated the portfolios. Instead we choose an innovative model $XGBoost + GA$ to make sense of the comprehensive input features and produce such output predictability to arbitrage in the context of PEAD. This is our main contribution to the literature.

Our test population possesses over 1000 of large cap, mid cap and small cap stocks sourced from 10 distinct industry sectors. Characteristics of any stock can fundamentally change over time as a company can develop from a relatively small cap growth stock into a large cap blue chip or decline from a market's darling to a penny stock. The underlying distribution of most companies' quarterly readings and the correlation between the readings and subsequent PEAD are constantly changing. All of these challenging factors have made it more meaningful to evaluate the dynamics of post-earnings drifts in the context of portfolios and we hope to capture an unseen collective trend of movement by certain stocks as triggered by their earnings release and other associative economic factors. With that in mind we've first discovered that our XGBoost + GA model's predicted results have successfully captured certain dynamics of PEAD of portfolio of stocks. By ranking the out-of-sample stocks according to their predicted $n$ day post earning cumulative abnormal returns, we've been able to consistently form portfolios using the top quantile which produce top positive returns and using the bottom quantile which produce low negative returns. A long-short market neutral strategy can be formed through longing the top portfolio and shorting the bottom one.










\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{"Average Actual Return of the top n stocks".png}
  \caption{Average Actual Return of the top n stocks}
\end{figure}


A 58\% classification success rate would allow us to build a profitable portfolio of stocks using the buy-and-hold scheme \cite{Chou2018}. However we would like not to focus on the profitability test and instead put our attention on a more important discovery from our experiments. Despite XGBoost's more superior performance against other tested models and a financially viable classification success rate of 58\%, we've recognised that on a purely point-by-point basis, a mean absolute error of 6.9\% in regression results can hardly be replied upon to perform accurate near term post-announcement price forecast on a single stock. On the other hand we've also recognised that since MAE is the mean of absolute errors it may not be the best error measure on the returns of an entire portfolio of stocks since movement of component stocks can offset each other within a portfolio. (\textit{Joseph's note: I don't know if it's appropriate to include the two preceding sentences}) By analysing the average return of groups of test stocks, We've subsequently discovered that by using predicted return of the 3035 individual test stocks between Q1 2016 and Q2 2018 as a proxy, we've been able to rank the test stocks in a way that the average actual return of the top n stocks (\(n  \in [2, 3035]\)) are almost always descending, and this is simply achieved by ranking the test stocks by their predicted returns from high to low. This phenomenon is best illustrated by Figure 3. In Figure 3, each point in the descending curve represents the average actual return of all the stocks up to the current point with the x-axis being index of the stocks, after all the test stocks have been ranked from high to low by the predicted 30 day post-earnings-announcement returns. This discovery indicates that we've successfully captured the relative positioning of individual stocks within the distribution of the 30 day post-announcement drifts from our test stocks. \textit{(Joseph's note: is the preceding sentence an accurate way of describing this discovery?)} Although we have not yet been able to rank individual test stock's return from high to low, we've been able to rank these stocks in a way that the average actual return of the top n stocks are almost always going from high to low as n increases.


Another way to look at how the test stocks are ranked by predicted returns is by looking at the average return of moving portfolios of 100 stocks. Again the test stocks have now been ranked from high to low according to their predicted returns. On Figure 4 each point on the graph represents the average actual return of the preceding 100 stocks and the graph is trending downward. This observation again shows that the predicted returns have correctly captured the rankings within the 3035 test stocks. This observation is further backed up by how correlated the test stocks' predicted returns are with the average actual return of the top n test stocks (once they are ranked by the predicted returns). We've run 20 rounds of predictions using XGBoost on the same training and test set and these two series of data have achieved an average R-square value as high as 0.7. This result consistently shows that we can form the highest-returning portfolios out of the sample population for buy-and-hold trading strategies and the lowest-returning portfolios for short selling strategies.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{"Average Actual Return of moving portfolios of 100 stocks".png}
  \caption{Average Actual Return of moving portfolios of 100 stocks}
\end{figure}

Further more, we've recognised that from practicality's point of view one can not form a 30 day buy-and-hold strategy on stocks that are available to buy only at different years, i.e. despite the seemingly significant results from our 3035 test stocks, these test stocks were sourced over two-and-a-half-year time horizon between Q1 2016 and Q2 2018 and hence we would not be able to practically form a portfolio from the top 100 stocks that is shown to have significantly higher risk-adjusted 30 day post-announcement returns than the bottom 100. As a result we've also looked at the average stock returns of only those test points from Q2 2018 after the stocks have been ranked by their predicted returns from high to low in Figure 5. The results are fairly consistent with those from the entire test set.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{"100 stock window size - Q2 2018 only".png}
  \caption{100 stock window size - Q2 2018 only}
\end{figure}

Lastly, in an attempt to explain our observations, we've looked at the test stocks more closely. We've discovered that although XGBoost's overall classification success rate on positive/negative returns stands at 58\%, the success rate for test stocks whose actual 30 day returns are positive is as high as 72\%. This new information indicates that when we rank the stocks by their predicted returns, we've more correctly placed positive-returning stocks at the upper end of the ranked series than the lower end and that echos with our earlier observations. However this observation doesn't explain why XGBoost has been able to 'rank' the stocks given that there is a clear descending trend in the graph with average returns of moving portfolios of 100 stocks.

\section{Conclusion}

To be written. There are more work to do and the eventual conclusions may well change.




%\subsubsection{\label{sec:level3}Third-level heading: Citations and Footnotes}





\medskip    % skip a line (create blank row)

\printbibliography[
heading=bibintoc,
title={bibliography}
] %Prints the entire bibliography with the titel "Whole bibliography"

\clearpage

%Filters bibliography
%\printbibliography[heading=subbibintoc,type=article,title={Articles only}]
%\printbibliography[type=book,title={Books only}]

%\printbibliography[keyword={physics},title={Physics-related only}]
%\printbibliography[keyword={latex},title={\LaTeX-related only}]

\end{document}
